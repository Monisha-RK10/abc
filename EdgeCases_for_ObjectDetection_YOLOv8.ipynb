{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ecf52bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries and YOLO model using internal python package for YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#edgeCase-1\n",
    "#occluded condition\n",
    "#description: Objects partially hidden behind other objects.\n",
    "#example: A pedestrian partially hidden by a parked car in an image.\n",
    "#effect: YOLO might miss the object or provide incorrect bounding boxes\n",
    "def apply_occlusion(image):\n",
    "    \"The function simulates occlusion by covering part of the image with a rectangle\"\n",
    "    #extract the height and width of the image, ignores channel\n",
    "    h, w = image.shape[:2] \n",
    "    #generate the coordinates of a rectangle randomly between (0 to w/2, 0 to h/2), these value can be changed\n",
    "    top_left = (random.randint(100, w//2), random.randint(100, h//2)) \n",
    "    #add the random width and height (i.e., between (50,200) these value can be changed) to the previously generated random \n",
    "    #points to get the bottom right of a rectangle\n",
    "    bottom_right = (top_left[0] + random.randint(500, 800), top_left[1] + random.randint(500, 800)) \n",
    "    #black rectangle to occlude\n",
    "    cv2.rectangle(image, top_left, bottom_right, (0, 0, 0), -1) \n",
    "    return image\n",
    "\n",
    "#edgeCase-2\n",
    "#extreme lighting conditions i.e., brightening and darkening\n",
    "#description: Objects in low-light or high-light conditions.\n",
    "#example: A person standing in the shadow of a building or under direct sunlight.\n",
    "#effect: The model might fail to detect objects in very bright or very dark areas of the image\n",
    "def apply_lighting(image, brightness_factor=1.5):\n",
    "    \"this function simulates the lighting conditions by adjusting brightness\"\n",
    "    #brightness factor adjusts the contrast of the image. It MULTIPLIES every pixel value in the image by this factor.\n",
    "    #if 1, pixels unchanged, >1 increases contrast, <1 decreases contrast\n",
    "    #beta (brightness shift): ADDS this value to each pixel to adjust brightness\n",
    "    #if 0 no change in brightness, >0 lightens the image, <0 darkens the image\n",
    "    return cv2.convertScaleAbs(image, alpha=brightness_factor, beta=0)\n",
    "\n",
    "#edgeCase-3\n",
    "#motion blur in a specific direction\n",
    "#description: Objects in motion that are blurred due to fast movement or camera shake.\n",
    "#example: A moving car, cyclist, or person with a blurred outline.\n",
    "#effect: YOLO may have difficulty detecting objects when they are motion-blurred, leading to reduced accuracy.\n",
    "def apply_motion_blur(image, kernel_size=5):\n",
    "    \"motion blur is linear blur in specific direction, simulating the effect of camera or object motion during image capture\"\n",
    "    #create an empty kernel (all zeros)\n",
    "    kernel = np.zeros((kernel_size, kernel_size))\n",
    "    #set the middle row of the kernel to ones (this represents the direction of motion)\n",
    "    kernel[kernel_size//2, :] = np.ones(kernel_size)\n",
    "    #kernel[:, kernel_size//2] = np.ones(kernel_size) for vertical\n",
    "    return cv2.filter2D(image, -1, kernel)\n",
    "\n",
    "#edgeCase-4\n",
    "#background noise\n",
    "#description: Objects appearing in noisy or low-quality images.\n",
    "#example: A noisy image with lots of pixel artifacts or poor resolution.\n",
    "#effect: YOLO may detect false positives or miss the correct object due to poor image quality.\n",
    "def apply_noise(image, noise_factor=0.02):\n",
    "    \"this function adds Gaussian noise to the image\"\n",
    "    row, col, ch = image.shape\n",
    "    #provide mean (0:no bias), std_dev (spread of noise), size\n",
    "    #gauss has values sampled from a normal distribution between -1 and 1, multiplying by 255 scales the noise to a reasonable range for pixel values \n",
    "    #(i.e., between -255 and 255).\n",
    "    gauss = np.random.normal(0, noise_factor ** 0.5, (row, col, ch))\n",
    "    #clip ensures that after adding the noise, all pixel values stay within the valid range of 0 to 255 (8 bit). \n",
    "    noisy = np.uint8(np.clip(image + gauss * 255, 0, 255))\n",
    "    return noisy\n",
    "\n",
    "#edgeCase-5\n",
    "#scale variations\n",
    "#description: Objects at different scales (sizes).\n",
    "#example: The same object in a close-up versus a distant view.\n",
    "#effect: YOLO may have difficulty detecting objects that are very large or very small in different image scales.\n",
    "def apply_scale(image, scale_factor=0.5):\n",
    "    \"this function scales the image by a given factor\"\n",
    "    h, w = image.shape[:2]\n",
    "    scaled_image = cv2.resize(image, (int(w * scale_factor), int(h * scale_factor)))\n",
    "    return scaled_image\n",
    "\n",
    "#edgeCase=6\n",
    "#rotation\n",
    "#description: Objects that are rotated in the image.\n",
    "#example: A car tilted at an angle or a rotated billboard.\n",
    "#effect: YOLO may struggle to correctly identify objects if they are rotated, especially when not trained on rotated images.\n",
    "def apply_rotation(image):\n",
    "    h,w,ch = image.shape\n",
    "    center = (w//2,h//2)\n",
    "    rotation_matrix = cv2.getRotationMatrix2D((w//2,h//2),45,0.5)\n",
    "    #determine the new bounding dimensions to prevent cropping\n",
    "    #determining the amount by which the horizontal axis (x-axis) is scaled due to the rotation\n",
    "    cos = np.abs(rotation_matrix[0, 0]) \n",
    "    #determining the amount by which the vertical axis (y-axis) is scaled due to the rotation\n",
    "    sin = np.abs(rotation_matrix[0, 1])\n",
    "    new_w = int((h * sin) + (w * cos))\n",
    "    new_h = int((h * cos) + (w * sin))\n",
    "    #adjust the rotation matrix to take the translation into account\n",
    "    rotation_matrix[0, 2] += (new_w / 2) - center[0]\n",
    "    rotation_matrix[1, 2] += (new_h / 2) - center[1]\n",
    "    #perform the affine transformation (rotation) with border handling\n",
    "    rotated_image = cv2.warpAffine(image, rotation_matrix, (new_w, new_h), borderMode=cv2.BORDER_CONSTANT)\n",
    "    return rotated_image\n",
    "\n",
    "#edgeCase-7\n",
    "#small objects (distant or tiny Objects) objects of various sizes in the same image.\n",
    "#description: Objects at different scales (sizes).\n",
    "#example: The same object in a close-up versus a distant view.\n",
    "#effect: YOLO may have difficulty detecting objects that are very large or very small in different image scales.\n",
    "def apply_small_objects(image, scale_factor=0.1):\n",
    "    \"this function simulates small objects by resizing the object to a smaller scale\"\n",
    "    h, w = image.shape[:2]\n",
    "    small_object = cv2.resize(image, (int(w * scale_factor), int(h * scale_factor)))\n",
    "    #generate random coordinate to place the top-left corner of the small object\n",
    "    small_object_x = np.random.randint(0, w - small_object.shape[1])\n",
    "    small_object_y = np.random.randint(0, h - small_object.shape[0])\n",
    "    #slice the original image[y:y+h,x:x+w] to place the small object\n",
    "    image[small_object_y:small_object_y + small_object.shape[0], small_object_x:small_object_x + small_object.shape[1]] = small_object\n",
    "    return image\n",
    "\n",
    "#edgeCase-8\n",
    "#scale variations (objects of various sizes in the same image)\n",
    "#description: Objects at different scales (sizes).\n",
    "#example: The same object in a close-up versus a distant view.\n",
    "#effect: YOLO may have difficulty detecting objects that are very large or very small in different image scales.\n",
    "def apply_scale_variations(image, scales=[0.1, 0.2, 0.8,1.2]):\n",
    "    \"this function simulate scale variations by resizing different parts of the image\"\n",
    "    h, w = image.shape[:2]\n",
    "    for scale in scales:\n",
    "        resized_image = cv2.resize(image, (int(w * scale), int(h * scale)))\n",
    "        if resized_image.shape[1] < w and resized_image.shape[0] < h: #resized image must be smaller than the original image\n",
    "            x_offset = np.random.randint(0, w - resized_image.shape[1])\n",
    "            y_offset = np.random.randint(0, h - resized_image.shape[0])\n",
    "            image[y_offset:y_offset + resized_image.shape[0], x_offset:x_offset + resized_image.shape[1]] = resized_image\n",
    "        else:\n",
    "            #resize the image further or handle the case when it doesn't fit\n",
    "            resized_image = cv2.resize(image, (w // 2, h // 2))  # or any other appropriate size\n",
    "    return image\n",
    "\n",
    "#edgeCase-9\n",
    "#description: Objects with unusual aspect ratios (long and thin or very wide objects).It is w/h (if >1:landscape, if<1:\n",
    "#portrait, if 1: square)\n",
    "#example: A long, stretched truck in a traffic scene or a very narrow pillar.\n",
    "#effect: YOLO might not detect such objects properly if the aspect ratio varies significantly from the training dataset.\n",
    "#aspect ratio variations (unusual proportions)\n",
    "def apply_aspect_ratio_variations(image, stretch_factor=1.5):\n",
    "    \"this function simulates aspect ratio (w,h) variations by stretching/compressing the image\"\n",
    "    h, w = image.shape[:2]\n",
    "    resized_image = cv2.resize(image, (w, int(h*stretch_factor)))  # Stretching width\n",
    "    return resized_image\n",
    "\n",
    "#edgeCase-10\n",
    "#object deformation (distorted object)\n",
    "#description: Objects that change shape or get distorted.\n",
    "#example: A crumpled or deformed cardboard box.\n",
    "#effect: Detection may fail if the object is not represented well in the training data, or if the deformation is substantial.\n",
    "def apply_object_deformation(image):\n",
    "    \"this function simulates object deformation by applying a warp (distortion)\"\n",
    "    #consider an example\n",
    "    #before transformation: top left (60,120), top right (460,120), bottom left (60,920), bottom right (460,920)\n",
    "    #after transformation: top left (60,120), top right (460,100) (moves vertically UP)\n",
    "    #after transformation: bottom left (50,920) (shifts horizontally LEFT), bottom right (460,900) (moves vertically UP because\n",
    "    #120-100 =20 so 920-20=900)\n",
    "    rows, cols, ch = image.shape\n",
    "    print('rows:height, columns:width', rows,cols)\n",
    "    pts1 = np.float32([[0, 0], [cols - 1, 0], [0, rows - 1]])\n",
    "    pts2 = np.float32([[0, 0], [cols-1, 0], [1000, rows-1]])\n",
    "    matrix = cv2.getAffineTransform(pts1, pts2)\n",
    "    deformed_image = cv2.warpAffine(image, matrix, (cols, rows))\n",
    "    return deformed_image\n",
    "\n",
    "#edgeCase-11\n",
    "#description: Objects that are close to each other, potentially overlapping.\n",
    "#example: Several cars in a parking lot, with little space between them.\n",
    "#effect: YOLO might struggle to distinguish between objects that are too close or overlapping.\n",
    "def apply_close_proximity_objects(image, object_bbox=(30,250,540,340), scale=0.4, proximity=100):\n",
    "    #extract the object's region of interest (ROI) using the bounding box (use paint to get coordinates)\n",
    "    image_resize = cv2.resize(image,(600,600))\n",
    "    x, y, w, h = object_bbox\n",
    "    obj_roi = image_resize[y:y+h, x:x+w]\n",
    "    #resize the ROI to a smaller scale (depends on the object in image otherwise you can keep the sacle as it is)\n",
    "    object_resized = cv2.resize(obj_roi,(int(w*scale),int(h*0.3)))\n",
    "    h_new,w_new = object_resized.shape[:2]\n",
    "    direction = np.random.choice(['right','above'])\n",
    "    \n",
    "    if direction == 'right':\n",
    "        print('right')\n",
    "        x_new = x + proximity - 30\n",
    "        y_new = y - 20\n",
    "    else:\n",
    "        print('above')\n",
    "        x_new = x \n",
    "        y_new = y - proximity \n",
    "        \n",
    "    print('x_new, w_new, w',x_new,w_new,w)\n",
    "    print('y_new, h_new, w',y_new,h_new,h) \n",
    "    \n",
    "    print('x_new + w_new',x_new + w_new,w)\n",
    "    print('y_new + h_new',y_new + h_new,h)\n",
    "    if x_new + w_new < w and  y_new + h_new < h:\n",
    "        print('here')\n",
    "        image[y_new:y_new+h_new,x_new:x_new+w_new] = object_resized   \n",
    "    return image\n",
    "\n",
    "#edgeCase-12\n",
    "#description: Objects that blend into or have a complex background.\n",
    "#example: A person in a crowd or an object against a similar color background (like a green car in a forest).\n",
    "#effect: YOLO may misclassify or fail to detect objects due to the complexity of the background.\n",
    "def apply_clutter(image,num_of_clutter=5):\n",
    "    h,w = image.shape[:2]\n",
    "    for _ in range(num_of_clutter):\n",
    "        clutter = np.random.randint(0,256,(int(w//4),int(h//2),3),dtype=np.uint8)\n",
    "        h_new,w_new = clutter.shape[:2]\n",
    "        x = np.random.randint(0,w-w_new)\n",
    "        y = np.random.randint(0,h-h_new)\n",
    "        if h_new<h and w_new<w:\n",
    "            image[y:y+h_new,x:x+w_new] = clutter\n",
    "    return image\n",
    "\n",
    "#edgeCase-13\n",
    "#adverse weather conditions: rain, snow\n",
    "#description: Environmental factors like rain, snow, fog, or dust that obscure objects.\n",
    "#example: A car on a foggy road or a pedestrian walking in heavy rain.\n",
    "#effect: YOLO may fail to detect objects due to reduced visibility and changes in the image texture.\n",
    "def apply_rain_effect(image, intensity=0.1):\n",
    "    h, w = image.shape[:2]\n",
    "    #create a 2D array of random integers between 0 and 256, with the same height and width as the image\n",
    "    #create a mask by comparing each pixel in the noise array to a threshold \n",
    "    #intensity: probability threshold for determining which pixels will be part of the rain effect but on average, \n",
    "    #for an intensity of 0.2, around 20% of the pixels will be part of the rain.\n",
    "    rain = np.random.randint(0, 256, (h, w), dtype=np.uint8) < (intensity * 256)\n",
    "    #set the pixel values at all 'True' positions in the rain mask to [255, 255, 255] (white), simulating raindrops.\n",
    "    image[rain] = [255, 255, 255]  #white rain list[]\n",
    "    return image\n",
    "def apply_snow_effect(image, intensity=0.005):\n",
    "    h, w = image.shape[:2]\n",
    "    num_snowflakes = int(h * w * intensity)\n",
    "    for _ in range(num_snowflakes):\n",
    "        #random position, ensuring the snowflake stays within bounds\n",
    "        x = np.random.randint(0 + 4, w - 4)  #x must be at least radius (4) away from the right edge\n",
    "        y = np.random.randint(0 + 4, h - 4)  #y must be at least radius (4) away from the bottom edge\n",
    "        size = np.random.randint(1, 5)  #random size for each snowflake (1 to 4 pixels)\n",
    "        cv2.circle(image, (x, y), size, (255, 255, 255), -1)  #draw the snowflake (circle)\n",
    "    return image\n",
    "\n",
    "#edgeCase-14\n",
    "#description: Objects on surfaces that cause reflections or glare.\n",
    "#example: Cars with shiny paint reflecting the surrounding environment or glass surfaces.\n",
    "#effect: The model might detect reflections as objects, leading to false positives.\n",
    "def apply_reflective_surface(image):\n",
    "    h,w = image.shape[:2]\n",
    "    glare = np.full((h//2,w//2),255,dtype=np.uint8)\n",
    "    h_new, w_new = glare.shape[:2]\n",
    "    x = np.random.randint(0,w-w_new)\n",
    "    y = np.random.randint(0,h-h_new)\n",
    "    if x+w_new<w and y+h_new<h:\n",
    "        cv2.rectangle(image,(x,y),(x+h_new, y+w_new),(255,255,255),-1)\n",
    "    else:\n",
    "        print('glare too big for the image!')\n",
    "    return image\n",
    "\n",
    "#edgeCase-15\n",
    "#description: Objects appearing in different colors in different positions.\n",
    "#example: An object of differnt colors, like red, blue, brown.\n",
    "#effect: YOLO might fail to detect objects if they look substantially different from what the model was trained on\n",
    "def apply_unexpected_objects(image, num_of_objects=5):\n",
    "    h,w = image.shape[:2]\n",
    "    object_size = np.random.randint(100,150)\n",
    "    for _ in range(num_of_objects):\n",
    "        x = np.random.randint(0,w-object_size)\n",
    "        y = np.random.randint(0,h-object_size)\n",
    "        color = np.random.randint(0,256,(3,)).tolist()\n",
    "        image[y:y+object_size,x:x+object_size] = color\n",
    "    return image\n",
    "\n",
    "#edgeCase-16\n",
    "#class imbalance (disproportionate class distribution in training)\n",
    "#description: The dataset has a disproportionate number of objects from different classes.\n",
    "#example: More images of pedestrians and fewer of cars in a traffic scene dataset.\n",
    "#effect: YOLO may overfit to the more frequent class, leading to poor performance on the less frequent class.\n",
    "def apply_class_imbalance(image, class_counts=[80, 5, 5]):\n",
    "    \"This function simulates class imbalance by adding more instances of one class\"\n",
    "    #for example, more instances of the first class\n",
    "    for _ in range(class_counts[0]):\n",
    "        #add the first class object (e.g., a car) multiple times\n",
    "        image = apply_small_objects(image, scale_factor=0.1)  # Smaller instances of car\n",
    "    for _ in range(class_counts[1]):\n",
    "        #add the second class object (e.g., a tree) fewer times\n",
    "        image = apply_scale_variations(image)\n",
    "    return image\n",
    "\n",
    "#select the edge case for an image\n",
    "def select_edgecase(image, edge_case):\n",
    "    \"switch-case logic to apply different transformations based on the edge case\"\n",
    "    if case_type == 'occlusion':\n",
    "        return apply_occlusion(image)\n",
    "    elif case_type == 'lighting':\n",
    "        return apply_lighting(image)\n",
    "    elif case_type == 'motion_blur':\n",
    "        return apply_motion_blur(image)\n",
    "    elif case_type == 'noise':\n",
    "        return apply_noise(image)\n",
    "    elif case_type == 'scale':\n",
    "        return apply_scale(image)\n",
    "    elif case_type == 'rotation':\n",
    "        return apply_rotation(image)\n",
    "    elif case_type == 'small_objects':\n",
    "        return apply_small_objects(image)\n",
    "    elif case_type == 'scale_objects':\n",
    "        return apply_scale_variations(image)\n",
    "    elif case_type == 'aspect_ratio':\n",
    "        return apply_aspect_ratio_variations(image)\n",
    "    elif case_type == 'object_deformation':\n",
    "        return apply_object_deformation(image)\n",
    "    elif case_type == 'close_proximity_objects':\n",
    "        return apply_close_proximity_objects(image)\n",
    "    elif case_type == 'clutter':\n",
    "        return apply_clutter(image)\n",
    "    elif case_type == 'rain':\n",
    "        return apply_rain_effect(image)\n",
    "    elif case_type == 'snow':\n",
    "        return apply_snow_effect(image)\n",
    "    elif case_type == 'reflective_surface':\n",
    "        return apply_reflective_surface(image)\n",
    "    elif case_type == 'unexpected_objects':\n",
    "        return apply_unexpected_objects(image)\n",
    "    else:\n",
    "        return image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac350676",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  Python>=3.10 is required, but Python==3.8.8 is currently installed \n",
      "\n",
      "0: 384x640 1 car, 85.6ms\n",
      "Speed: 4.4ms preprocess, 85.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mD:\\ComputerVision_Projects\\ObjectDetection_YOLOv8\\Edge_Test_Cases\\output\\predict18\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#select the model YOLOv8 from YOLO\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "#load the test image\n",
    "image = cv2.imread('D:\\ComputerVision_Projects\\ObjectDetection_YOLOv8\\Edge_Test_Cases\\input\\car.jpg')\n",
    "    \n",
    "#selct an edge case to test\n",
    "case_type = 'clutter' # Change to other types like 'occlusion', 'motion_blur', etc.\n",
    "    \n",
    "#apply transformation based on the edge case\n",
    "transformed_image = select_edgecase(image, case_type)\n",
    "    \n",
    "#run YOLO object detection on the transformed image\n",
    "#save the results in the below path\n",
    "save_path =r'D:\\ComputerVision_Projects\\ObjectDetection_YOLOv8\\Edge_Test_Cases\\output'\n",
    "#perform the inference on the pre-trained weights\n",
    "model.predict(transformed_image, save = True, project=save_path,conf=0.5, iou=0.4) \n",
    "  \n",
    "#visualize the transformed image\n",
    "cv2.imshow('Transformed Image', transformed_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db565417",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results at confidence score 0.5 and IoU at 0.4\n",
    "#original image - 0.88\n",
    "\n",
    "# 1. occlusion - 0.87\n",
    "# 2. lighting - 0.88\n",
    "# 3. motion_blur - 0.77\n",
    "# 4. noise - 0.76\n",
    "# 5. scale - 0.88\n",
    "# 6. rotation - 0.74\n",
    "# 7. small_objects - 0.81, did not detect the small car at scale 0.1 of the original image\n",
    "# 8. scale_objects at scale [0.1, 0.2, 0.8,1.2] - predicted truck (wrong) with 0.74 and did not predict other cars\n",
    "# 9. aspect_ratio - 0.65\n",
    "# 10. object_deformation - predicted airplane (wrong) with 0.54\n",
    "# 11. background_clutter - 0.56\n",
    "# 12(A) multiple_objects_in_close_proximity (right) - 'Right' car with 0.66 and original car with 0.84\n",
    "# 12(b) multiple_objects_in_close_proximity (above) - 'Above' car with 0.82 and original car with 0.87\n",
    "# 13. class_imbalance \n",
    "# 14(A). rain - predicted truck (wrong) with 0.78\n",
    "# 14(B). snow - no prediction\n",
    "# 15. reflective_surface - predicted a part of the car with 0.93 and other part as truck with 0.66\n",
    "# 16. unexpected_objects (color variation) - 0.83"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1265b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#observations\n",
    "# 1. strong performance on occlusion and lighting variations\n",
    "# 2. challenges with motion blur, noise, and rotation - may need more training dataset with these varitions/data augmentation\n",
    "# 3. difficulty detecting small objects and scale variations - small objects are often challenging for YOLO models\n",
    "# 4. aspect ratio and object deformation sensitivity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7cb572",
   "metadata": {},
   "outputs": [],
   "source": [
    "#suggestion\n",
    "# 1. data augmentation - Ideal for increasing dataset size and generalizing the model.\n",
    "# 2. fine tuning\n",
    "# 3. hyperparameter tuning such as IoU, confidence score\n",
    "# 4. other architectures designed for small object detection such as (a) YOLOv8’s smaller scale layers or (b) Faster R-CNN with \n",
    "#Feature Pyramid Networks (FPN)\n",
    "# 5. post-processing adjustments - for missed detections (scale variations) adjust NMS thresholds\n",
    "#to refine detections where objects are close in scale or overlap significantly\n",
    "# 6. anchor box adjustments\n",
    "# 7. synthetic data (GAN) - Ideal for rare or extreme scenarios that are hard to collect.\n",
    "# 8. ensemble models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
